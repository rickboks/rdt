#!/bin/bash
# Load config file
source "$RDT_DIR/config" || exit

session_string="$RANDOM$RANDOM$RANDOM$RANDOM"
output_dir="/tmp/rdt/$session_string"
success_file="$output_dir/success"
subs_file="$RDT_DIR/subs"

ydl_download(){
	local title="$2"
	youtube-dl -q -o "${download_dir}/${title}" "$1" &>/dev/null
	return $?
}

num(){
	[ -z "$1" ] && echo "0" || wc -l <<< "$1"
}

download_urls(){
	IFS=$'\n' read -r -d '' -a all_titles <<< "$(jq -r ".title" <<< "$1")"
	all_urls="$(jq -r -c ".urls" <<< "$1")"
	local num=${#all_titles[@]}

	: > "$success_file"

	for ((i=0;i<num;i++)); do
		local filename="$RANDOM-$RANDOM-$RANDOM-$RANDOM"
		[ -z "$scrape" ] && echo "${filename};${all_titles[i]}" >> "${download_dir}/titles.dat"
		local urls=$(sed -n $((i+1))p <<< "$all_urls" | jq -r '.[]')
		IFS=$'\n'; while read -r url; do
			case "$2" in 
				wget)
					wget -q -O "${download_dir}/$filename" "$url"
					[ $? -eq 0 ] && echo "$i" >> "$success_file" && break ;;
				*)
					download_url "$url" "$filename" 
					[ $? -eq 0 ] && echo "$i" >> "$success_file" && break ;;
			esac
		done <<< $urls & # This loop is run in the background
	done

	wait # Wait for all downloads to finish before continuing
	succeeded=$(<$success_file)
	echo "> Successful downloads: $(num "$succeeded")/$num"
}

download_url (){
	local url="$1"
	local title="$2"

	case "$url" in
		https://gfycat.com*)
			download_gfycat "$url" "$title" ;;
		*)
			ydl_download "$url" "$title" ;;	
	esac

	return $?
}

download_gfycat(){
	local url="$1"
	local title="$2"
	if ! ydl_download "$url" "$title"; then
		get_redgifs_url "$url"
		ydl_download "$redgifs_url" "$title"
		return $?
	fi

	return 0 
}

get_redgifs_url() {
	local url="$1"
	local id=${url##*\/}
	redgifs_url=$(curl "https://api.redgifs.com/v1/gfycats/$id" 2> /dev/null | jq -r ".gfyItem.webmUrl")
}

auth(){
	local auth_url="https://www.reddit.com/api/v1/authorize\
		?client_id=${client_id}&response_type=code&state=${session_string}\
		&redirect_uri=${redirect_uri}&duration=permanent&scope=read"

	eval "$browser \"$auth_url\""
}

get_auth_token(){
	echo "Getting authentication token ..."
	local temp=$(curl -X POST -d "grant_type=password&username=${username}&password=${password}"\
		--user "${client_id}:${client_sec}" -A "${user_agent}" https://www.reddit.com/api/v1/access_token 2> /dev/null)
	local auth_error=$(jq -r ".error" <<< "$temp")
	if [ "$auth_error" != "null" ]; then
		echo "Authentication error: $auth_error"
		exit
	fi
	access_token="$(jq -r ".access_token" <<< $temp)"
	token_type="$(jq -r ".token_type" <<< $temp)"
}

download_next_page(){
	download_dir="$1"

	local get_posts_url="https://oauth.reddit.com/r/${subreddit}/${listing}?raw_json=1&limit=${limit}"
	[ "$listing" = "top" -o "$listing" = "controversial" ] && get_posts_url="$get_posts_url&t=${period}"

	[ -f "$output_dir/after.tmp" ] && after=$(<"$output_dir/after.tmp") || after=""
	[ -n "$after" ] && get_posts_url="${get_posts_url}&after=$after"

	local json=$(curl -s -S -H "Authorization: ${token_type} ${access_token}" -A "${user_agent}" "$get_posts_url")
	local error=$(jq -r ".error" <<< "$json" 2> /dev/null || echo "this subreddit probably doesn't exist") 
	if [ "$error" != "null" ]; then
		echo "Error: $error"
		exit
	fi

	local posts=$(jq -r ".data.children[].data | del (.all_awardings)" <<< "$json")
	jq -r ".data.after" <<< "$json" > "$output_dir/after.tmp"

	local images=$(jq -c -r ". | 
		select ( .post_hint == \"image\" ) | {title: .title,
			urls: [
			.preview.images[].variants.mp4.source.url,
			.preview.images[].variants.gif.source.url,
			.preview.images[].source.url
			]} " <<< "$posts")

	local gallery_images=$(jq -c -r ".title as \$title | 
		select (.is_gallery == true) | 
			.media_metadata[] | select(.e == \"Image\") | 
			{title: \$title, urls : [(.p | max_by( .x ) | .u)]}" <<< "$posts" )

	local hosted_videos=$(jq -c -r ". | 
		select ( .post_hint == \"hosted:video\" ) | {title: .title,
		urls: [(\"https://reddit.com/\" + .id)] }" <<< "$posts")

	local rich_videos=$(jq -c -r ". | 
		select ( .post_hint == \"rich:video\" ) | {
			title: .title,
			urls: [
			(if .preview.reddit_video_preview.fallback_url then (\"https://reddit.com/\" + .id) else null end),
			.url
			] | map(select(.!= null)) }" <<< "$posts")

	local links=$(jq -c -r ". | 
		select ( .post_hint == \"link\" )| {title: .title, 
			urls: [
			(if .preview.reddit_video_preview.fallback_url then
			(\"https://reddit.com/\" + .id)
		elif (.url | test(\"https?://imgur.com\")) and .preview.images[].source.url then
			.preview.images[].source.url
		else
			.url end)]}" <<< "$posts")


	local unknown=$(jq -c -r ". | select (.post_hint == null)" <<< "$posts" )
	local num_unknown=$(num "$unknown")
	[ $num_unknown -gt 0 ] && echo "There are $num_unknown posts with unknown type"

	local videos_combined="$(printf "%s\n%s\n%s" "${hosted_videos}" "${rich_videos}" "${links}" | grep -v "^$")"
	local images_combined="$(printf "%s\n%s" "$images" "$gallery_images")"

	local num_images=$(num "$images_combined")
	local num_videos=$(num "$videos_combined")

	[ $num_images -gt 0 ] && (echo "Downloading ${num_images} images ..."; download_urls "$images_combined" "wget")
	[ -z "$skip_videos" ] && [ $num_videos -gt 0 ] && (echo "Downloading ${num_videos} videos/links ..."; download_urls "$videos_combined" "ydl")

	echo "Done downloading page."
}

prepare_directories(){
	[ ! -d "$save_dir" ] && mkdir -p "$save_dir"
	rm -rf "$output_dir"
	mkdir -p "$output_dir/current"
	mkdir -p "$output_dir/next"
}

cleanup() {
	echo "Cleaning up ..."
	kill $(pgrep -g0 | grep -v $$) &>/dev/null
	rm -r $output_dir
}

ask_listing_and_period(){
	[ -z "$listing" ] && listing="$(printf 'hot\nnew\nrising\ntop\ncontroversial' | dmenu)"
	case "$listing" in
		hot | new | rising)
			period="" ;;
		top | controversial)
			[ -z "$period" ] && period="$(printf 'all\nyear\nmonth\nweek\nday' | dmenu)"
			case "$period" in
				all | year | month | week | day) : ;;
				*) echo "Invalid period: $period"; exit ;;
			esac ;;
		*) echo "Invalid listing: $listing"; exit ;;
	esac
}

ask_subreddit(){ 
	subreddit="$(cat "$subs_file" | dmenu)" 
}

browse(){
	echo "Downloading initial buffer of ${limit} items."
	download_next_page "$output_dir/current"

	local done=0
	while [ "$done" -eq 0 ]; do
		download_next_page "$output_dir/next" &
		local mpv_output=$(python "$RDT_DIR/mpv-rdt.py" "$output_dir/current" "$save_dir" 2> /dev/null | tail -1) 

		if [ "$mpv_output" = "quit" ]; then
			local done=1
			echo "Exiting."
			cleanup; exit
		fi

		wait

		rm -f "$output_dir/current/"*
		mv "$output_dir/next/"* "$output_dir/current"
	done
}

scrape(){
	echo "Scraping ${limit} items."
	local total=$limit
	local remaining=$limit
	[ $remaining -gt 99 ] && limit=100

	while [ $remaining -gt 0 ]; do
		echo "Remaining: ${remaining}"
		download_next_page "$scrape_dir"
		remaining="$((remaining-$limit))"
	done
}

show_help(){
	echo "Command line options:

-h
Show this message.

-s [subreddit]

-l [listing] 
[listing] should be one of {hot, new, top, rising controversial}

-p [period] 
[period] should be one of {all, year, month, week, day}

-n [limit] 
For browsing, [limit] is the number of posts per page. 
For scraping, [limit] is the total number of posts to scrape.

-x [directory]
Enable scraping mode. [directory] is the directory to download to.

-V
Do not download videos and links.
"
}

trap "cleanup; exit" INT

# Parse command line arguments
while getopts s:p:l:n:x:f:Vha opt; do
	case "$opt" in
		a) auth; exit;;
		s) subreddit="$OPTARG" ;;
		p) period="$OPTARG" ;;
		l) listing="$OPTARG" ;;
		n) limit="$OPTARG"
			case "$limit" in
				*[^0-9]*) echo "Error: $opt must be an integer"; exit
			esac
			;;
		x) 
			scrape_dir="$OPTARG"
			mkdir -p "$scrape_dir"
			[ ! -d "$scrape_dir" ] && echo "Invalid output dir: $scrape_dir" && exit
			scrape=1
			;;
		V)
			skip_videos=1
			;;
		h)
			show_help; exit
			;;
		f) 
			subs_file="$OPTARG" ;;
	esac
done

[ -z "$subreddit" ] && ask_subreddit
ask_listing_and_period

echo "Subreddit: $subreddit"
echo -n "Listing: $listing"
[ -n "$period" ] && echo ", $period" || echo

get_auth_token
prepare_directories

if [ -z "$scrape" ]; then
	browse
else
	scrape
fi
